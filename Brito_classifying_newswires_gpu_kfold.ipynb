{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brito-classifying-newswires-problem3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brito-rafa/DeepLearningIntro/blob/master/Brito_classifying_newswires_gpu_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5g-ZqYHwpHF6",
        "colab_type": "code",
        "outputId": "16a9321b-f9b2-4758-da77-7fd300adb55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWJVle2KpHGI",
        "colab_type": "code",
        "outputId": "5dbc6fee-9fe4-4b17-8d9c-3ad48f97d2f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ps0HORKxpHGP",
        "colab_type": "code",
        "outputId": "68ad3e7c-f415-4285-a01f-e45d8c5a56af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "KGRocb2WpHGV",
        "colab_type": "code",
        "outputId": "92255508-91ae-4862-b9b8-ad00231a7816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "4LPHNajypHGc",
        "colab_type": "code",
        "outputId": "8f1f4d32-1d34-4235-f59f-14787625a0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "train_data[10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "vbJkHo7KpHGk",
        "colab_type": "code",
        "outputId": "d409ad2b-55ba-4993-bcaf-6f3a75cae725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2jkNgayYpHGp",
        "colab_type": "code",
        "outputId": "f90fca99-1dd3-44eb-f143-b25f320fd84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "decoded_newswire"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "5B2tmXOdpHGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "metadata": {
        "id": "ruR1L6BupHGu",
        "colab_type": "code",
        "outputId": "79a385b8-e6b6-4600-9e1d-b2218317e306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_labels[10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "Gou2c0K3pHGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81Q-j8dPpHG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ThD74WdpHG6",
        "colab_type": "code",
        "outputId": "6fc131fa-7b84-467d-b703-cf097b4f570c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "print (one_hot_train_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TxXHvN9vpHG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8b465a21-c8ae-4c48-b4df-f1cd0e210386"
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fHDtKeyBpHHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbtjB2aqpHHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        },
        "outputId": "cd940940-f7b7-4a9e-d236-aaa2ccd73f1c"
      },
      "cell_type": "code",
      "source": [
        "k = 4\n",
        "num_val_samples = len(x_train) // k\n",
        "num_epochs = 8\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "  print ('processing fold #', i)\n",
        "  x_val_data = x_train[i * num_val_samples: (i +1 ) * num_val_samples]\n",
        "  y_val_targets = one_hot_train_labels[i * num_val_samples: (i +1 ) * num_val_samples]\n",
        "\n",
        "  partial_x_train_data = np.concatenate ([x_train[:i * num_val_samples], x_train[i * num_val_samples:]], axis=0)\n",
        "  partial_y_train_data = np.concatenate ([one_hot_train_labels[:i * num_val_samples], one_hot_train_labels[i * num_val_samples:]], axis=0)\n",
        "  \n",
        "  start_time = time.time()\n",
        "  with tf.device('/gpu:0'):\n",
        "    model.fit(partial_x_train_data,\n",
        "          partial_y_train_data,\n",
        "          epochs=num_epochs,\n",
        "          batch_size=512)\n",
        "  time_in_seconds = (time.time() - start_time)\n",
        "  time_in_seconds = round(time_in_seconds,4)\n",
        "  print ('Time in seconds, Google Colaboratory: ', time_in_seconds)\n",
        "  val_mse, val_mae = model.evaluate(x_val_data, y_val_targets)\n",
        "  all_scores.append(val_mae)\n",
        "  print ('Accuracy:', val_mae)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/8\n",
            "8982/8982 [==============================] - 2s 172us/step - loss: 2.4410 - acc: 0.5245\n",
            "Epoch 2/8\n",
            "8982/8982 [==============================] - 1s 64us/step - loss: 1.3727 - acc: 0.7027\n",
            "Epoch 3/8\n",
            "8982/8982 [==============================] - 1s 69us/step - loss: 1.0472 - acc: 0.7745\n",
            "Epoch 4/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.8259 - acc: 0.8258\n",
            "Epoch 5/8\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.6563 - acc: 0.8612\n",
            "Epoch 6/8\n",
            "8982/8982 [==============================] - 1s 63us/step - loss: 0.5219 - acc: 0.8893\n",
            "Epoch 7/8\n",
            "8982/8982 [==============================] - 1s 61us/step - loss: 0.4251 - acc: 0.9115\n",
            "Epoch 8/8\n",
            "8982/8982 [==============================] - 1s 64us/step - loss: 0.3427 - acc: 0.9246\n",
            "Time in seconds, Google Colaboratory:  6.0067\n",
            "2245/2245 [==============================] - 0s 112us/step\n",
            "Accuracy: 0.9425389755276635\n",
            "processing fold # 1\n",
            "Epoch 1/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.2870 - acc: 0.9353\n",
            "Epoch 2/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.2469 - acc: 0.9411\n",
            "Epoch 3/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.2063 - acc: 0.9466\n",
            "Epoch 4/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1830 - acc: 0.9513\n",
            "Epoch 5/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.1665 - acc: 0.9517\n",
            "Epoch 6/8\n",
            "8982/8982 [==============================] - 1s 67us/step - loss: 0.1543 - acc: 0.9536\n",
            "Epoch 7/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1457 - acc: 0.9523\n",
            "Epoch 8/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.1352 - acc: 0.9549\n",
            "Time in seconds, Google Colaboratory:  4.7481\n",
            "2245/2245 [==============================] - 0s 97us/step\n",
            "Accuracy: 0.9657015590200445\n",
            "processing fold # 2\n",
            "Epoch 1/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1273 - acc: 0.9557\n",
            "Epoch 2/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1212 - acc: 0.9554\n",
            "Epoch 3/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.1208 - acc: 0.9549\n",
            "Epoch 4/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1146 - acc: 0.9559\n",
            "Epoch 5/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.1120 - acc: 0.9528\n",
            "Epoch 6/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1088 - acc: 0.9578\n",
            "Epoch 7/8\n",
            "8982/8982 [==============================] - 1s 67us/step - loss: 0.1094 - acc: 0.9548\n",
            "Epoch 8/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1049 - acc: 0.9551\n",
            "Time in seconds, Google Colaboratory:  4.7465\n",
            "2245/2245 [==============================] - 0s 90us/step\n",
            "Accuracy: 0.9710467706013363\n",
            "processing fold # 3\n",
            "Epoch 1/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1049 - acc: 0.9582\n",
            "Epoch 2/8\n",
            "8982/8982 [==============================] - 1s 67us/step - loss: 0.1043 - acc: 0.9551\n",
            "Epoch 3/8\n",
            "8982/8982 [==============================] - 1s 64us/step - loss: 0.1010 - acc: 0.9561\n",
            "Epoch 4/8\n",
            "8982/8982 [==============================] - 1s 64us/step - loss: 0.1014 - acc: 0.9568\n",
            "Epoch 5/8\n",
            "8982/8982 [==============================] - 1s 64us/step - loss: 0.1012 - acc: 0.9549\n",
            "Epoch 6/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.0974 - acc: 0.9554\n",
            "Epoch 7/8\n",
            "8982/8982 [==============================] - 1s 66us/step - loss: 0.1006 - acc: 0.9549\n",
            "Epoch 8/8\n",
            "8982/8982 [==============================] - 1s 65us/step - loss: 0.0972 - acc: 0.9551\n",
            "Time in seconds, Google Colaboratory:  4.6953\n",
            "2245/2245 [==============================] - 0s 99us/step\n",
            "Accuracy: 0.9648106904231626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yvnlm4QjpHHX",
        "colab_type": "code",
        "outputId": "1843595f-3996-44ca-eca4-1c2bdc08c613",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.98764628548762257, 0.77693677651807869]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZMiQqRgpHHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
        "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
      ]
    },
    {
      "metadata": {
        "id": "3S-kfmiEpHHb",
        "colab_type": "code",
        "outputId": "9cd4d241-bd37-4204-b5aa-b37d0f163d3e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18477292965271594"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "A-ugCzzmpHHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating predictions on new data\n",
        "\n",
        "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
        "predictions for all of the test data:"
      ]
    },
    {
      "metadata": {
        "id": "4GuUTT3CpHHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wu2PGWgBpHHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each entry in `predictions` is a vector of length 46:"
      ]
    },
    {
      "metadata": {
        "id": "zBqov4qwpHHi",
        "colab_type": "code",
        "outputId": "9aa33d29-b92d-4b7d-a022-2fa72f270e82",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "6S1HTom8pHHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The coefficients in this vector sum to 1:"
      ]
    },
    {
      "metadata": {
        "id": "zWtJBdgYpHHl",
        "colab_type": "code",
        "outputId": "53ff4f13-d96f-4213-e9e3-c349f7390da1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sum(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99999994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "lEWQ-faNpHHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The largest entry is the predicted class, i.e. the class with the highest probability:"
      ]
    },
    {
      "metadata": {
        "id": "Wxmj9AJ1pHHp",
        "colab_type": "code",
        "outputId": "075e80f4-7a6f-475e-bc98-b111d4db77dd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "HDDgCt7opHHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A different way to handle the labels and the loss\n",
        "\n",
        "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
      ]
    },
    {
      "metadata": {
        "id": "w_tVw8B1pHHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fxlB5YtKpHHt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
        "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
      ]
    },
    {
      "metadata": {
        "id": "7EBDaszdpHHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nu20rqOzpHHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
      ]
    },
    {
      "metadata": {
        "id": "DBbFDEYdpHHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## On the importance of having sufficiently large intermediate layers\n",
        "\n",
        "\n",
        "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
        "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
        "46-dimensional, e.g. 4-dimensional."
      ]
    },
    {
      "metadata": {
        "id": "snJng1LwpHHw",
        "colab_type": "code",
        "outputId": "6d0c5393-cb80-47fc-967d-59c58c489a65",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8ce7cdb9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "Og90UTzBpHHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
        "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
        "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
        "of it."
      ]
    },
    {
      "metadata": {
        "id": "mrq-zpMHpHHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Further experiments\n",
        "\n",
        "* Try using larger or smaller layers: 32 units, 128 units...\n",
        "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
      ]
    },
    {
      "metadata": {
        "id": "WMo1DqPupHH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
        "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
        "probability distribution over the N output classes.\n",
        "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
        "probability distributions output by the network, and the true distribution of the targets.\n",
        "* There are two ways to handle labels in multi-class classification:\n",
        "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
        "function.\n",
        "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
        "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
        "intermediate layers that are too small."
      ]
    }
  ]
}